{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Light GBM model Optimization and Logging\n",
    "- Split into training and testing sets\n",
    "- Optimize model parameters\n",
    "- Automate training across n=6 countries and k=2 datasets\n",
    "- Log results on MLflow server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Multiple outputs per notebook cell\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# random_state for different processes\n",
    "RANDOM_STATE = 221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_rows_horizon(df, timeframe='7day'):\n",
    "    ''' \n",
    "    Input: Dataframe, Timeframe (in to_timedelta format)\n",
    "    Output: Outputs number of rows in dataframe to reach timeframe\n",
    "    '''\n",
    "    return len(df.loc[df['day'] > (df['day'].max() - pd.to_timedelta('7day'))])\n",
    "\n",
    "# validate test\n",
    "def trainval_test_split(df):\n",
    "    '''\n",
    "    Input: Dataframe from features prep\n",
    "    Output: Train and final test sets with a 7 day window\n",
    "    '''\n",
    "    train_validate = df.loc[df['day'] < (df['day'].max() - pd.to_timedelta('7day'))]\n",
    "    final_test = df.loc[df['day'] > (df['day'].max() - pd.to_timedelta('8day'))]\n",
    "\n",
    "    return train_validate, final_test\n",
    "\n",
    "# train validate\n",
    "def load_train_test(df, drop_index=True):\n",
    "    ''' \n",
    "    Input: Dataframe from features prep\n",
    "    Output: Data prepared for hyperparameter search\n",
    "    '''\n",
    "    X = (df.drop(columns=['load_actual', 'country', 'day']) \n",
    "           .dropna()\n",
    "           .reset_index()\n",
    "           )\n",
    "    if drop_index:\n",
    "        X = X.drop(columns='utc_timestamp')\n",
    "     \n",
    "    y = (df.reset_index(drop=True)[['load_actual']])\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "- Creating train/test and validation sets, then ensure windows are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure validation set is last 7 days; min date: 2019-04-23 00:00:00+00:00, max date: 2019-04-30 00:00:00+00:00\n",
      "Ensure train/test set excludes last 7 days; min date: 2016-01-01 01:45:00+00:00, max date: 2019-04-22 23:45:00+00:00\n",
      "Ensure validation set is last 7 days; min date: 2019-04-23 00:00:00+00:00, max date: 2019-04-30 00:00:00+00:00\n",
      "Ensure train/test set excludes last 7 days; min date: 2016-01-01 07:00:00+00:00, max date: 2019-04-22 23:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "cd = Path.cwd()\n",
    "data_dir = str(cd.parents[1])\n",
    "upsampled = data_dir + '/datasets/country_energy/load_wthr_downsample_update.pickle'\n",
    "downsampled = data_dir + '/datasets/country_energy/load_wthr_upsample_update.pickle'\n",
    "\n",
    "# Xu - upsampled\n",
    "with open(upsampled, 'rb') as f:\n",
    "    Xu = pickle.load(f)\n",
    "\n",
    "# Xd - downsampled\n",
    "with open(downsampled, 'rb') as f:\n",
    "    Xd = pickle.load(f)\n",
    "\n",
    "# Create test/train and validation set. Ensure dates are correct\n",
    "final_test_u = Xu.loc[Xu['day'] > (Xu['day'].max() - pd.to_timedelta('8day'))] # 8 days is selected because last day 2019-4-30 has 0 hours\n",
    "Xu_check = Xu.loc[Xu['day'] < (Xu['day'].max() - pd.to_timedelta('7day'))]\n",
    "print(f'Ensure validation set is last 7 days; min date: {final_test_u.index.min()}, max date: {final_test_u.index.max()}')\n",
    "print(f'Ensure train/test set excludes last 7 days; min date: {Xu_check.index.min()}, max date: {Xu_check.index.max()}')\n",
    "\n",
    "final_test_d = Xd.loc[Xd['day'] > (Xd['day'].max() - pd.to_timedelta('8day'))]\n",
    "Xd_check = Xd.loc[Xd['day'] < (Xd['day'].max() - pd.to_timedelta('7day'))]\n",
    "print(f'Ensure validation set is last 7 days; min date: {final_test_d.index.min()}, max date: {final_test_d.index.max()}')\n",
    "print(f'Ensure train/test set excludes last 7 days; min date: {Xd_check.index.min()}, max date: {Xd_check.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization and logging for MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast_pipeline.lightgbm_forecasting_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated forecasting per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def automated_forecast(datasets, dataset_names, iter_per_model=25, nested_windows=10, experiment_name='No experiment name given'):\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # Initialize server\n",
    "        mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        for name, data in zip(dataset_names, datasets):\n",
    "            mlflow.start_run(run_name=f'{name} Country Energy Forcast')\n",
    "\n",
    "            # Group data, begin process for each group in data\n",
    "            country_data = data.groupby('country')\n",
    "\n",
    "            for country, data in country_data: \n",
    "                #with mlflow.start_run(nested=True, run_name=f\"Country: {country}\"):\n",
    "                hyperparam_opt(data, country, iterations=iter_per_model, nested_windows=nested_windows)  \n",
    "               \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run automated forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Params for testing\n",
    "# datasets = [Xd, Xu] \n",
    "# dataset_names = ['1hour', '15min']\n",
    "# iterations = [15, 30]\n",
    "\n",
    "mlflow.end_run()\n",
    "#mlflow.start_run()\n",
    "automated_forecast(datasets=[Xu],\n",
    "                   dataset_names='15min Intervals',\n",
    "                   iter_per_model=15, # 20\n",
    "                   experiment_name=f'Added Variables',\n",
    "                   nested_windows=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
